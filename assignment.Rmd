---
title: 'DATA 622: Homework #4'
author: "Karim Hammoud and Michael Munguia"
date: "11/12/2021"
output: html_document
---

```{r setup, include=FALSE}
set.seed(123)

knitr::opts_chunk$set(
  cache.extra = knitr::rand_seed, echo = TRUE, warning = FALSE, message = FALSE
  )

library(tidyverse)
library(corrplot)

gh_repo <- "https://raw.githubusercontent.com/Izote/DATA622-Assignment4/main/"

adhd <- read_csv(
  str_c(gh_repo, "adhd_data.csv"), col_types = c("c", rep("d", 53))
  ) %>% 
  mutate(Index = row_number())
```

# Section 1

> Conduct a thorough Exploratory Data Analysis (EDA) to understand the dataset.

We can start our exploration of the data by first viewing summary statistics for each numeric variable with a call to the `summary` function:

```{r}
summary(adhd)
```

We'll create an `explore` function to streamline visualization of the data set, as shown below:

```{r}
adhd_cols <- str_subset(colnames(adhd), "^ADHD")
md_cols <- str_subset(colnames(adhd), "^MD")

demo_cols <- colnames(adhd)[!colnames(adhd) %in% c(adhd_cols, md_cols)]
demo_cols <- demo_cols[!demo_cols %in% c("Initial", "Suicide")]

explore <- function(columns, stat, n_cols = 4) {
  adhd %>%
    pivot_longer(cols = all_of(columns)) %>%
    mutate(across("Suicide", factor)) %>%
    ggplot(aes(x = value, color = Suicide, fill = Suicide, group = Suicide)) + 
    geom_density(alpha = 0.3, stat = stat) +
    facet_wrap(~ name, scales = "free", ncol = n_cols)
}
```

There are three broad categories of variables recorded in the data - general intake data and two sets coming from an ADHD self-report and mood disorders (MD) questionnaire. We will visualize the distribution of responses within subsets in succession, starting with the non-questionnaire variables, below. Because we will want to eventually predict suicide attempts with this data, emphasis is placed on understanding the data through that lens. 

```{r}
explore(demo_cols[demo_cols != "Abuse"], stat = "count")
```

There are a few interesting takeaways from this subset of the data. There seems to be no true pattern across ages, though one might argue there are more older patients reporting no suicide attempts - which is intuitive, as younger suicidal patients would have a higher likelihood of not reaching an old age than their non-suicidal peers. 

Variables measuring a history of substance use across a spectrum of abstinence to dependence show a decrease in prevalence of non-suicidal patients the further they veer towards substance dependence. Neither cocaine nor THC have as a pronounced a change in this regard as other substances. 

It initially seems like psychiatric medication self-reporting might be insightful, however, 67% of patients lack a response for this variable so it is ultimately unhelpful. Sex appears to have a relationship with suicide attempts in the sample with more men than women having never attempted suicide. More women in the sample appear to have made a suicide attempt than not with the percentage being about 42% and 57% for men and women respectively. Race may be an important factor, but the data set is heavily inbalanced and representative of black (100) and white (72) patients out of the overall 175 patients.

Visualizing the results from the ADHD questionnaire are difficult to summarize without the context of the question-content itself, but we can see that certain questions seemed to illicit higher responses from either group. Unlike the results from the MD assessment, there isn't as clear a throughline from question-to-question. Broadly speaking, a higher score in the assessment seems to come with higher incidences of suicide attempt.

```{r}
explore(adhd_cols, stat = "count")
```

Visualizing the responses from the MD questionnaire reveal that in most cases there is a pronounced increase in the proportion of patients having attempted suicide pending a "Yes" answer to the given question.

```{r warning=FALSE}
explore(md_cols, stat = "count")
```

# Section 2

> Use a clustering method to find clusters of patients here. Whether you choose to use k-means clustering or hierarchical clustering is up to you as long as you reason through your work. You are free to be creative in terms of which variables or some combination of those you want to use. Can you come up with creative names for the profiles you found?



# Section 3

> Let’s explore using Principal Component Analysis on this dataset. You will note that there are different types of questions in the dataset: column: E-W: ADHD self-report; column X – AM: mood disorders questionnaire, column AN-AS: Individual Substance Misuse; etc. You could just use ONE of the sets of questionnaire, for example, you can conduct PCA on the ADHD score, or mood disorder score, etc. Please reason through your work as you decide on which sets of variables you want to use to conduct Principal Component Analysis. What did you learn from the PCA? Can you comment on which question may have a heavy bearing on the score?


We chose to perform principal components analysis (PCA) on the ADHD data. We start by utilizing a 

```{r}
pr_out <- prcomp(adhd[5:23], scale = TRUE)
pr_out
```

The center and scale components correspond to the means and standard deviations of the variables that were used for scaling prior to implementing PCA.

```{r}
names(pr_out)

pr_out$center
```

The rotation matrix provides the principal component loadings. Each column of `pr_out$rotation` contains the corresponding principal component loading vector.

```{r}
pr_out$rotation
```

We can plot the first two principal components as follows:

```{r}
biplot(pr_out, scale = 0)
```

We can access these standard deviations as follows:

```{r}
pr_out$sdev
```

The variance explained by each principal component is obtained by squaring the standard deviation. To compute the proportion of variance explained by each principal component, we simply divide the variance explained by each principal component by the total variance explained by all four principal components. From there, we can obtain and plot the cumulative proportion of variance explained by each principal component:

```{r}
pr_var <- pr_out$sdev^2
pve <- pr_var/sum(pr_var)

cumsum(pve)
```

Now lets plot the Proportion of Variance Explained, and Proportion of Variance Explained.

```{r}
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained")
```

```{r}
plot(cumsum(pve), xlab = "Principal Component", ylab = " Proportion of Variance Explained")
```

# Section 4

> Assume you are modeling whether a patient attempted suicide (column AX). This is a binary target variable. Please use Gradient Boosting to predict whether a patient attempts suicides. Please use whatever boosting approach you deem appropriate. But please be sure to walk us through your steps.

In the following sections, we will follow conduct predictive modeling with both a Gradient Boosting based model and a support vector machine (SVM) based model to predict suicide attempts. Even though it is included in the prompt for the next section, we will do some feature engineering here so that both models can benefit from that work. There were a few observations during data exploration that informed this process, which reolved around creating a new series of binary variables valued 0-1 to either reduce the variables utilized or simplify them into a tidier format. Our logic is summarized as follows:

* Despite the lack of diversity in the data set, the `Race` variable reveals a higher proportion of white participants have attempted suicide than black participants. The "Hispanic" label is also a broad heritage rather than a specific race, making it inappropriate to record here. Because there were so few participants labeled as something other than black or white, this variable is reduced to the binary variable `White`. 

* Because all the substance-use variables measured the same behavior (i.e. a spectrum of non-user, user and addict) these could each be reduced to a binary representing whether the participant is simply a user of the given controlled substance. Missing values could essentially be seen as the participant being unwilling to self-report their usage - a behavior unlikely for non-users and so these could be imputed as `0`. We experimented with this as a single case-statement driven variable (i.e. a single variable for usage of any substances), but keeping each substance separate seemed to help the models better generalize.

* The `Abuse` variable was converted to a tidy variable that represents whether a participant self-reported as the victim of any kind of abuse. Contrasting to the substance-abuse variables, this seemed to produce better results as a single catch-all.

* The two assessments had their many variables reduced to a pair of performance scores based on the maximum possible scores eluded to in the data dictionary.

In order to setup our model, the prepped data needed to be sampled. Because the number of suicidal versus non-suicidal participants was imbalanced (about 28% where `Suicide = 1`), we setup the training set to include a more deliberate sampling across both groups. This does not result in a perfect 50/50 split - that would not be an unreasonable expectation given this data set. It does, however, provide a better set of inputs than if we naively sampled the entire data set to reserve 20% of it for testing.

```{r}
adhd2 <- adhd %>%
  mutate(
    White = if_else(Race == 1, 1, 0),
    
    Alcohol = if_else(Alcohol != 0, 1, 0),
    THC = if_else(THC != 0, 1, 0),
    Cocaine = if_else(Cocaine != 0, 1, 0),
    Stimulants = if_else(Stimulants != 0, 1, 0),
    `Sedative-hypnotics` = if_else(`Sedative-hypnotics` != 0, 1, 0),
    Opioids = if_else(Opioids != 0, 1, 0),
    
    Victim = if_else(`Abuse` != 0, 1, 0),
    
    ADHD = `ADHD Total` / 72,
    MD = `MD TOTAL` / 17
  ) %>%
  select(
    Index,
    Suicide,
    White,
    Alcohol, THC, Cocaine, Stimulants, `Sedative-hypnotics`, Opioids,
    Victim,
    ADHD,
    MD
    ) %>%
  drop_na()
```

We configure the model to ignore the `Index` variable created for cross-referencing observations and choose AdaBoost as our method via the `distribution` parameter. This will provide us with as the target variable is a binary outcome. We setup 10-fold cross validation as well, and establish that we will 1,000 separate iterations. The `interaction.depth` parameter specifies the depth of the trees were generate in the process where `interaction.depth = 1` would be a stump.

```{r}
library(gbm)

set.seed(3)

training <- bind_rows(
  slice_sample(filter(adhd2, Suicide == 0), prop = .80),
  slice_sample(filter(adhd2, Suicide == 1), prop = .80)
  )
testing <- anti_join(adhd2, training, by = "Index")

gbm_mod <- gbm(
  Suicide ~ . -Index,
  distribution = "bernoulli",
  data = training,
  n.trees = 1000,
  interaction.depth = 3,
  shrinkage = 0.01,
  cv.folds = 10,
)
```

The `gbm` library provides a method for us to output the best performing number of trees/iterations based on our cross-validation. We store this number below with the side effect of viewing the results of the iterative process.

```{r}
best_gbm <- gbm.perf(gbm_mod)
```

The prediction output we generate below will not arrive immediately as the output class. For that, we create a threshold and apply it to the prediction output to assign a class, much like with logistic regression via `glm`.

```{r}
gbm_train_pred <- predict(
  gbm_mod, newdata = select(training, -Suicide),
  n.trees = best_gbm,
  type = "response"
  )

gbm_test_pred <- predict(
  gbm_mod, newdata = select(testing, -Suicide),
  n.trees = best_gbm,
  type = "response"
  )

threshold <- 0.40

training$GBM <- as.double(gbm_train_pred > threshold)
testing$GBM <- as.double(gbm_test_pred > threshold)
```

After defining a confusion matrix function, we can produce the results below for both the training and test data sets.

```{r}
conf_mat <- function(df, prediction_column) {
  table("expected" = df[["Suicide"]], "predicted" = df[[prediction_column]])
}

conf_mat(training, "GBM")
```

```{r}
conf_mat(testing, "GBM")
```

The training and testing results are 81% and 63% accuracy respectively. While initially promising, this does not seem like a model that generalizes particularly well to new data. In the following section, however, we will the result of another type of model that will we will favor for this particular prediction task.

# Section 5

> Using the same target variable (suicide attempt), please use support vector machine to model this. You might want to consider reducing the number of variables or somehow use extracted information from the variables. This can be a really fun modeling task!

The `e1071` library, chosen for this task for its familiarity, requires the target variable be stored as a factor in this case and so we apply that additional transformation atop our earlier data preparation.

With a training data set in hand, we can pass a formula for our model to the `best.svm` method and have it fit an optimal SVM-based model given our situation. The resulting `C-classification` type and `radial` kernel are selected as a result - this accords with general results from manual experimentation we conducted outside of this report.

We exclude both the `Index` and earlier `GBM` prediction variables.


```{r}
library(e1071)

training <- mutate(training, Suicide = factor(Suicide))
testing <- mutate(testing, Suicide = factor(Suicide))

set.seed(3)

best_svm <- best.svm(
  x = Suicide ~ . -Index -GBM,
  data = training
)

best_svm
```

Given the model above, we can generate some predictions using the usual `predict` method for SVM models and store that on our data sets. This allows us to easily create confusion matrices and evaluate the results.

```{r, echo=FALSE}
training$`SVM` <- predict(best_svm, newdata = training)
testing$`SVM` <- predict(best_svm, newdata = testing)

conf_mat(training, "SVM")

```

```{r}
conf_mat(testing, "SVM")
```

Based on the confusion matrices for the training and testing data respectively displayed above, we calculate about 85% and 75% accuracy respectively. This training model performs comparably as the earlier GBM, but requires significantly less computation time. 

As would be anticipated, there is a drop in accuracy between training and testing data but in having experimented with other kernel options, it looks like the optimal model provided by the `best.svm` method generalizes relatively well by comparison. It does, unfortunately, seem to have a training false negative rate of about 41% and about 50% in testing. Not knowing the eventually goal/outcome for the model's usage, a model with a lower false negative rate and higher false positive rate might be preferable if one is wishing to be cautious in targeting suicidal patients for intervention/treatment.